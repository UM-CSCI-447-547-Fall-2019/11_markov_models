{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic 11: Markov Models\n",
    "\n",
    "## Reading: Bishop, Chapter 13\n",
    "\n",
    "## IC11A Weather Prediction\n",
    "Imagine that you're tasked with predicting the weather in a remote village, without access to the internet, no radar, etc.  In fact, let's imagine that the only thing that you have to go on is a long-term record of qualitative observations of the weather: sunny, raining, snowing.  \n",
    "\n",
    "**It's raining today.  Using this dataset, create a 5 day forecast.  Specifically, report a probability that it will be sunny, rainy, or snowing for each of the next five days.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "weather_data = np.load('weather.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"virga.jpg\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Markov Models\n",
    "One way of doing this, of course, would be just to come up with a naive statistical model.  In particular, we could model a given day's weather as a simple categorical distribution, with the probabilities derived from maximum likelihood estimation over the dataset.  \n",
    "$$ P(W=w) = \\begin{cases} \\frac{m_{sun}}{m} &\\text{if }w=\\text{sunny} \\\\\n",
    "                           \\frac{m_{rain}}{m} &\\text{if }w=\\text{raining} \\\\\n",
    "                           \\frac{m_{snow}}{m} &\\text{if }w=\\text{snowy} \\end{cases} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course, this model is going to be very poor.  **What assumption have we made that is probably invalid for the weather?**\n",
    "\n",
    "A more reasonable way to proceed is to model weather as \n",
    "$$ P(W_t|W_{t-1},W_{t-2},\\ldots) \\neq P(W_t), $$\n",
    "which is to say that *today's weather depends on yesterday's weather, and the day before, and the day before all the way back to the beginning of time*.  While this is a highly expressive model, it isn't practical.  **Why not?**.  \n",
    "\n",
    "There is, of course, a middle ground.  We could condition our weather probability on a fixed and finite number of previous states.  In particular, when we condition exclusively on the previous state, we get something called a *first order Markov Model*, named after [Andrey Markov](https://en.wikipedia.org/wiki/Andrey_Markov), the Russian statistician.  A physical system where the future is independent of the past *given the present* is called a Markov process, or is said to be Markovian.  \n",
    "$$\n",
    "P(W_t|W_{t-1}) = P(W_t|W_{t-1},W_{t-2},\\ldots)\n",
    "$$\n",
    "If we have a trained model, then we have the ability to answer the question: given that today is sunny, what is the probability that tomorrow will be rainy?  **What are the parameters of this model?  How many are there for the weather example?**\n",
    "\n",
    "## IC11B Training a Markov Model\n",
    "How should we train a Markov Model?  Training proceeds very much like in the case of the naive model: simply count the number of times that (e.g.) a sunny day transitioned to a rainy day, and divide by the total number of sunny days.  This is most easily done by creating a matrix $M$, where the *from* state is represented by the rows, and the *to* state by the columns, and the entry represents the total number of times that transition occurred:\n",
    "$$\n",
    "M = \\begin{bmatrix} m_{\\text{sun} \\rightarrow \\text{sun}} & m_{\\text{sun} \\rightarrow \\text{rain}} &  m_{\\text{sun} \\rightarrow \\text{snow}} \\\\\n",
    "m_{\\text{rain} \\rightarrow \\text{sun}} & m_{\\text{rain} \\rightarrow \\text{rain}} &  m_{\\text{rain} \\rightarrow \\text{snow}} \\\\\n",
    "m_{\\text{snow} \\rightarrow \\text{sun}} & m_{\\text{snow} \\rightarrow \\text{rain}} &  m_{\\text{snow} \\rightarrow \\text{snow}} \n",
    "\\end{bmatrix},\n",
    "                    $$\n",
    "Once we've counted all the different transitions, we can create a matrix of transition probabilities (typically named $A$) by dividing each row in $M$ by the sum over that row.\n",
    "\n",
    "**Compute the transition matrix $A$**.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Generating Data\n",
    "One interesting consequence of having explicitly formulated a probabilistic model of something is that we have the capacity to *simulate* new data.  So for example, once I have a trained $A$, I could simulate a weather record that is statistically consistent with the model by doing something like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Comment me out to use the results from above!!!\n",
    "A = np.random.rand(3,3)\n",
    "A = A/(A.sum(axis=1)[:,np.newaxis])\n",
    "###\n",
    "\n",
    "current_state = 0 # Sunny\n",
    "state_record = [current_state]\n",
    "m_simulated = 100\n",
    "\n",
    "# Create 100 synthetic observations\n",
    "for i in range(m_simulated):\n",
    "    # Extract the current state's row from the transition matrix\n",
    "    transition_row = A[current_state]\n",
    "    \n",
    "    # Generate a new state by sampling from possible states with \n",
    "    # probability given by that row\n",
    "    new_state = np.random.choice([0,1,2],p=transition_row)\n",
    "    \n",
    "    # Store and update the current state\n",
    "    state_record.append(new_state)\n",
    "    current_state=new_state\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, we can make *probabilistic* forecasts about what the weather will be.  This is especially simple for a Markov Model, as\n",
    "$$\n",
    "P(W_{t+1}) = P(W_{t+1}|W_t) P(W_t) = P(W_t) A\n",
    "$$\n",
    "So for example, if we know for a fact that today is sunny, then $P(W_t) = [1,0,0]$, and we have that"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_today = np.array([[1,0,0]])\n",
    "forecast_tomorrow = weather_today @ A\n",
    "print(forecast_tomorrow)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obviously, this just pulls the first row from the transition matrix, but we can chain these computations to get the weather after two days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecast_twoday = forecast_tomorrow @ A\n",
    "print(forecast_twoday)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In fact, we could come up with long term forecasts by repeated application of the transition matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_today = np.array([[1,0,0]])\n",
    "forecasts = [weather_today]\n",
    "days_to_forecast = 10\n",
    "for t in range(days_to_forecast):\n",
    "    forecast = forecasts[-1] @ A\n",
    "    forecasts.append(forecast)\n",
    "\n",
    "forecasts = np.array(forecasts).squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(forecasts[:,0],label='P(Sunny)')\n",
    "plt.plot(forecasts[:,1],label='P(Rainy)')\n",
    "plt.plot(forecasts[:,2],label='P(Snowy)')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interesting to note that repeated application yields a state probability that converges.  **What do you suppose the steady states for the probabilities are?  How could you check against your dataset?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 Markov Processes as a model for language\n",
    "There are, of course, many other phenomena that could conceivably be modelled as a Markov Process (perhaps a higher-order Markov process).  Language is one example: the next word in a sentence is clearly dependent upon the word that immediately precedes it\n",
    "\n",
    "Now, let's try a more interesting Markov model, in which we simulate the linguistic styles of various authors with recognizable styles.  To begin with, we'll train a Markov model with the complete works of William Shakespeare.  Much as in the weather example, training will consist of finding the transition probabilities from one word to the next.  As such, we'll compute prior probabilities representing the relative frequency of words in the body of work, and we'll also compute a gigantic set of transition probabilities from each word to all the other words.  First, we'll read in the Shakespeare text,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division,print_function\n",
    "\n",
    "import numpy as np\n",
    "import string\n",
    "from collections import Counter\n",
    "import re\n",
    "import json\n",
    "import unicodedata\n",
    "\n",
    "sequence_shakespeare = []\n",
    "file = open('t8.shakespeare.txt','r')\n",
    "for line in file:\n",
    "    line.strip('\\n')\n",
    "    if line[:2] == '  ':\n",
    "        line_words = re.findall(r\"[\\w']+|[.,!?;]\",line)\n",
    "        line_words = [str(w).lower() for w in line_words if not w.isupper() and not w.isdigit()] \n",
    "\n",
    "        sequence_shakespeare.extend(line_words)\n",
    "        \n",
    "print (' '.join(sequence_shakespeare[:100]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "He goes on an on.  Now, we'll create a First-order Markov Model built for this purpose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FirstOrderMarkovModel(object):\n",
    "    \"\"\"\n",
    "    Implements a bigram model for text generation and classification\n",
    "\n",
    "    Attributes:\n",
    "        terminal_characters: a set of punctuation that shouldn't fall at the\n",
    "                             beginning of a sentence, but should end one.\n",
    "\n",
    "    Methods:\n",
    "        build_transition_matrices: builds a dictionary of word frequency\n",
    "        generate_phrase: generates a phrase by randomly sampling from \n",
    "                         word frequencies\n",
    "        compute_log_likelihood: computes the logarithm of the likelihood\n",
    "                                for a specified phrase\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    terminal_characters = ['.','?','!']\n",
    "\n",
    "    def __init__(self,sequence):\n",
    "        \"\"\"\n",
    "        sequence: an ordered list of words corresponding to the training set\n",
    "        \"\"\"\n",
    "\n",
    "        self.order=1\n",
    "        self.sequence = sequence\n",
    "        self.sequence_length = len(sequence)\n",
    "        self.transitions = [{}]\n",
    "        for i in range(self.order):\n",
    "            self.transitions.append({})\n",
    "        \n",
    "\n",
    "    def build_transition_matrices(self):\n",
    "        \"\"\"\n",
    "        Builds a set of nested dictionaries of word probabilities\n",
    "        \"\"\"\n",
    "\n",
    "        for i in range(self.sequence_length):\n",
    "            word = self.sequence[i]\n",
    "            if word in self.transitions[0]:\n",
    "                self.transitions[0][word] += 1\n",
    "            else:\n",
    "                self.transitions[0][word] = 1\n",
    "\n",
    "        transition_sum = float(sum(self.transitions[0].values()))\n",
    "        for k,v in self.transitions[0].items():\n",
    "            self.transitions[0][k] = v/transition_sum\n",
    "\n",
    "        for i in range(self.sequence_length-1):\n",
    "            word = self.sequence[i]\n",
    "            next_word = self.sequence[i+1]\n",
    "            if word in self.transitions[1]:\n",
    "                if next_word in self.transitions[1][word]:\n",
    "                    self.transitions[1][word][next_word] += 1\n",
    "                else:\n",
    "                    self.transitions[1][word][next_word] = 1\n",
    "            else:\n",
    "                self.transitions[1][word] = {}\n",
    "                self.transitions[1][word][next_word] = 1\n",
    "\n",
    "        for k_1,tdict in self.transitions[1].items():\n",
    "            key_sum = float(sum(self.transitions[1][k_1].values()))\n",
    "            for k_2,v in tdict.items():\n",
    "                self.transitions[1][k_1][k_2] = v/key_sum \n",
    "\n",
    "    def generate_phrase(self):\n",
    "        \"\"\"\n",
    "        Take a random sample from the probability distribution.  Terminate\n",
    "        when a period, question mark, or exclamation point is drawn.\n",
    "        \"\"\"\n",
    "\n",
    "        w_minus_1 = '?'\n",
    "        while w_minus_1 in self.terminal_characters:\n",
    "            w_minus_1 = np.random.choice([*self.transitions[0].keys()],replace=True,p=[*self.transitions[0].values()])\n",
    "        phrase = w_minus_1+' '\n",
    "        while w_minus_1 not in self.terminal_characters:\n",
    "            w_minus_1 = np.random.choice([*self.transitions[1][w_minus_1].keys()],replace=True,p=[*self.transitions[1][w_minus_1].values()])\n",
    "            phrase += w_minus_1+' '\n",
    "        return phrase\n",
    "\n",
    "    def compute_log_likelihood(self,phrase,lamda=0.0,unknown_probability=1e-5):\n",
    "        \"\"\"\n",
    "        Return the log-probability of a given phrase (entered as a string)\n",
    "        lambda: regularization factor for unseen transitions\n",
    "        unknown_probability: probability mass of a word not in the dictionary.\n",
    "        \"\"\"\n",
    "\n",
    "        words_in = phrase.split()\n",
    "        \n",
    "        w_i = words_in[0]\n",
    "        try: \n",
    "            log_prob = np.log(self.transitions[0][w_i])\n",
    "        except KeyError:\n",
    "            log_prob = np.log(unknown_probability)\n",
    "        for w in words_in[1:]:\n",
    "            try:\n",
    "                fjk = 0\n",
    "                if w in self.transitions[1][w_i]:\n",
    "                    fjk = self.transitions[1][w_i][w]\n",
    "                log_prob += np.log((1-lamda)*fjk + lamda*self.transitions[0][w])\n",
    "                w_i = w\n",
    "            except KeyError:\n",
    "                log_prob += np.log(unknown_probability)\n",
    "        return log_prob\n",
    "\n",
    "# Create First order Markov Model for shakespeare sequence\n",
    "mm_shakespeare = FirstOrderMarkovModel(sequence_shakespeare)\n",
    "mm_shakespeare.build_transition_matrices()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last line has built the transition matrices, which are actually not matrices in this implementation, but instead dictionaries that store only entries for which there appears a transition.  For example, 'the' is never followed by another 'the', so it would be a waste to explicitly keep track of a zero probability case.  This is actually true for the vast majority of word pairs, so not keeping a 30000 by 30000 matrix is advantageous.  Let's take a look at these datastructures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mm_shakespeare.transitions[0]\n",
    "\n",
    "sum([i for i in mm_shakespeare.transitions[0].values()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This first dictionary is just the absolute word frequencies in the corpus: the probability of drawing a word from the shakespeare text, regardless of the preceding word.  These probabilities, of course, add up to one.\n",
    "\n",
    "The second dictionary is more interesting.  It gives us the rows of a sparse transition matrix.  For example, what's the probability of transitioning from \"cat\" to any other word?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mm_shakespeare.transitions[1]['cat']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this model trained, we can do interesting things like generate a synthetic phrase, in exactly the same way that we did for the weather data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mm_shakespeare.generate_phrase()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The feel is right, if not exactly sensible!  These models are great at capturing tone and style, but not so much the meaning.  \n",
    "\n",
    "Another thing that we can do is to use our statistical model to evaluate the probability of new examples.  For example, if I wanted to evaluate how probable it was that Shakespeare wrote the phrase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_string = 'to be or not to be'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I would just evaluate the prior probability on 'to' then multiply that by $P(be|to)$ then the probability of $P(or|be)$ and so on.  In practice we'll use log probabilities to avoid underflow:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_like_shakespeare = mm_shakespeare.compute_log_likelihood(test_string,lamda=0.01,unknown_probability=1e-10)\n",
    "print (log_like_shakespeare)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These aren't that interesting on their own.  A better use for these log-likelihoods is as a classification scheme.  If I had another statistical model built upon a corpus of text, I could compute the likelihood for both and decide which writer produced the text.  **I posit that you've already done this.  What kind of model and dataset have we done something similar on in the past?**  \n",
    "\n",
    "One contemporary goldmine of idiosyncratic text is the twitter account of Donald J. Trump.  Let us create a model for his tweets.  First, we read in the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_trump = []\n",
    "file = open('trump_tweets.json')\n",
    "tweet_list = json.loads(file.read())\n",
    "for t in tweet_list:\n",
    "    tweet = unicodedata.normalize('NFKD',t['text'])\n",
    "    tweet = re.sub(r'\\w+:\\/{2}[\\d\\w-]+(\\.[\\d\\w-]+)*(?:(?:\\/[^\\s/]*))*', '', tweet)\n",
    "    line_words = re.findall(r\"[\\w']+|[.,!?;]\",tweet)\n",
    "    line_words = [str(w).lower() for w in line_words if not w.isdigit()] \n",
    "\n",
    "    sequence_trump.extend(line_words)\n",
    "\n",
    "print (' '.join(sequence_trump[:100]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like Shakespeare, he *also* goes on and on.  Now, we can use exactly the same code to generate the Trump Markov Model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mm_trump = FirstOrderMarkovModel(sequence_trump)\n",
    "mm_trump.build_transition_matrices()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can, again, generate a random phrase using this model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mm_trump.generate_phrase()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now we can evaluate the likelihood of \"to be or not to be\" for Trump"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_like_trump = mm_trump.compute_log_likelihood(test_string,lamda=0.01,unknown_probability=1e-10)\n",
    "\n",
    "trump_factor = np.exp(log_like_trump - log_like_shakespeare)\n",
    "print(trump_factor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'to be or not to be' is 4 times more likely to be Shakespeare, which is not very strong evidence.  Let's try again with a longer phrase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_string = 'to be or not to be , that is the question'\n",
    "log_like_shakespeare = mm_shakespeare.compute_log_likelihood(test_string,lamda=0.01,unknown_probability=1e-10)\n",
    "log_like_trump = mm_trump.compute_log_likelihood(test_string,lamda=0.01,unknown_probability=1e-10)\n",
    "\n",
    "trump_factor = np.exp(log_like_trump - log_like_shakespeare)\n",
    "print(trump_factor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is much more likely to be Shakespeare, now that we have more data.  Conversely, let's try something that Trump actually said, but was certainly not in the tweet corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_string = \"\"\"my uncle explained that to me many, many years ago, \n",
    "the power and that was 35 years ago; he would explain the power of what's \n",
    "going to happen and he was right—who would have thought?, but when you look \n",
    "at what's going on with the four prisoners—now it used to be three, \n",
    "now it’s four—but when it was three and even now, I would have said \n",
    "it's all in the messenger; fellas, and it is fellas because, you know, \n",
    "they don't, they haven’t figured that the women are smarter right now than the men, \n",
    "so, you know, it’s gonna take them about another 150 years—but the Persians are \n",
    "great negotiators, the Iranians are great negotiators, so, and they, they just killed, they just killed us.\"\"\"\n",
    "\n",
    "log_like_shakespeare = mm_shakespeare.compute_log_likelihood(test_string,lamda=0.01,unknown_probability=1e-10)\n",
    "log_like_trump = mm_trump.compute_log_likelihood(test_string,lamda=0.01,unknown_probability=1e-10)\n",
    "\n",
    "trump_factor = np.exp(log_like_trump - log_like_shakespeare)\n",
    "print(trump_factor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is much more likely to have been written by Trump.\n",
    "\n",
    "Finally, these models in which words are only dependent on their immediate predecessor are called 'bigram' models.  They aren't particularly good at generating realistic text.  Better results can be had by considering the previous two or more words, albeit with a commensurate increase in cost and tendency to overfit.  **Now, instead of having $\\mathcal{O(v^2)}$, where $v$ is the size of the vocabulary, how many parameters will we have?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SecondOrderMarkovModel(object):\n",
    "    \"\"\"\n",
    "    Implements a trigram model for text generation\n",
    "\n",
    "    Attributes:\n",
    "        terminal_characters: a set of punctuation that shouldn't fall at the\n",
    "                             beginning of a sentence, but should end one.\n",
    "\n",
    "    Methods:\n",
    "        build_transition_matrices: builds a dictionary of word frequency\n",
    "        generate_phrase: generates a phrase by randomly sampling from \n",
    "                         word frequencies\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    terminal_characters = ['.','?','!']\n",
    "\n",
    "    def __init__(self,sequence):\n",
    "        \"\"\"\n",
    "        sequence: an ordered list of words corresponding to the training set\n",
    "        \"\"\"\n",
    "\n",
    "        self.order=2\n",
    "        self.sequence = sequence\n",
    "        self.sequence_length = len(sequence)\n",
    "        self.transitions = [{}]\n",
    "        for i in range(self.order):\n",
    "            self.transitions.append({})\n",
    "\n",
    "    def build_transition_matrices(self):\n",
    "        \"\"\"\n",
    "        Builds a set of nested dictionaries of word probabilities\n",
    "        \"\"\"\n",
    "\n",
    "        for i in range(self.sequence_length):\n",
    "            word = self.sequence[i]\n",
    "            if word in self.transitions[0]:\n",
    "                self.transitions[0][word] += 1\n",
    "            else:\n",
    "                self.transitions[0][word] = 1\n",
    "\n",
    "        transition_sum = float(sum(self.transitions[0].values()))\n",
    "        for k,v in self.transitions[0].items():\n",
    "            self.transitions[0][k] = v/transition_sum\n",
    "\n",
    "        for i in range(self.sequence_length-1):\n",
    "            word = self.sequence[i]\n",
    "            next_word = self.sequence[i+1]\n",
    "            if word in self.transitions[1]:\n",
    "                if next_word in self.transitions[1][word]:\n",
    "                    self.transitions[1][word][next_word] += 1\n",
    "                else:\n",
    "                    self.transitions[1][word][next_word] = 1\n",
    "            else:\n",
    "                self.transitions[1][word] = {}\n",
    "                self.transitions[1][word][next_word] = 1\n",
    "\n",
    "        for k_1,tdict in self.transitions[1].items():\n",
    "            key_sum = float(sum(self.transitions[1][k_1].values()))\n",
    "            for k_2,v in tdict.items():\n",
    "                self.transitions[1][k_1][k_2] = v/key_sum \n",
    "\n",
    "        for i in range(self.sequence_length-2):\n",
    "            word = self.sequence[i]\n",
    "            next_word = self.sequence[i+1]\n",
    "            next_next_word = self.sequence[i+2]\n",
    "            if word in self.transitions[2]:\n",
    "                if next_word in self.transitions[2][word]:\n",
    "                    if next_next_word in self.transitions[2][word][next_word]:\n",
    "                        self.transitions[2][word][next_word][next_next_word] += 1\n",
    "                    else:\n",
    "                        self.transitions[2][word][next_word][next_next_word] = 1\n",
    "                else:\n",
    "                    self.transitions[2][word][next_word] = {}\n",
    "                    self.transitions[2][word][next_word][next_next_word] = 1\n",
    "            else:\n",
    "                self.transitions[2][word] = {}\n",
    "                self.transitions[2][word][next_word] = {}\n",
    "                self.transitions[2][word][next_word][next_next_word] = 1\n",
    "                    \n",
    "        for k_1,tdict_1 in self.transitions[2].items():\n",
    "            for k_2,tdict_2 in tdict_1.items():\n",
    "                key_sum = float(sum(tdict_2.values()))\n",
    "                for k_3,v in tdict_2.items():\n",
    "                    self.transitions[2][k_1][k_2][k_3] = v/key_sum\n",
    "\n",
    "    def generate_phrase(self):\n",
    "        \"\"\"\n",
    "        Take a random sample from the probability distribution.  Terminate\n",
    "        when a period, question mark, or exclamation point is drawn.\n",
    "        \"\"\"\n",
    "        w_minus_2 = '?'\n",
    "        while w_minus_2 in self.terminal_characters:\n",
    "            w_minus_2 = np.random.choice([*self.transitions[0].keys()],replace=True,p=[*self.transitions[0].values()])\n",
    "        w_minus_1 = np.random.choice([*self.transitions[1][w_minus_2].keys()],replace=True,p=[*self.transitions[1][w_minus_2].values()])\n",
    "        phrase = w_minus_2 + ' ' + w_minus_1 + ' '\n",
    "        while w_minus_1 not in self.terminal_characters:\n",
    "            t_mat = self.transitions[2][w_minus_2][w_minus_1]\n",
    "            w_i = np.random.choice([*t_mat.keys()],replace=True,p=[*t_mat.values()])\n",
    "            phrase += w_i + ' '\n",
    "            w_minus_2 = w_minus_1\n",
    "            w_minus_1 = w_i\n",
    "        return phrase\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mm_shakespeare = SecondOrderMarkovModel(sequence_shakespeare)\n",
    "mm_shakespeare.build_transition_matrices()\n",
    "mm_trump = SecondOrderMarkovModel(sequence_trump)\n",
    "mm_trump.build_transition_matrices()\n",
    "\n",
    "shakespeare_or_trump = ['Shakespeare','Trump']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use these to play a game: we'll randomly select from the two models, generate a phrase and try to decide who said it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [mm_shakespeare,mm_trump]\n",
    "index = np.random.randint(2)\n",
    "models[index].generate_phrase()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(index)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
